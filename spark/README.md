# About the course
Big data systems, including Cloud Computing and parallel data processing frameworks, emerge as enabling technologies in managing and mining 
the massive amount of data across hundreds or even thousands of commodity servers in datacenters. This course exposes students to both the 
theory and hands-on experience of this new technology. The course will cover the following topics. (1) Basic concepts of Cloud Computing and 
production Cloud services; (2) MapReduce - the de facto datacenter-scale programming abstraction - and its open source implementation of 
Hadoop. (3) Apache Spark - a new generation parallel processing framework - and its infrastructure, programming model, cluster deployment, 
tuning and debugging, as well as a number of specialized data processing systems built on top of Spark.

### Intended Learning Outcomes
On successful completion of the course, students will be able to:

1. Build a Hadoop Ecosystem.
2. Manage the data over Spark systems.
3. Apply the knowledge of Spark and Hadoop Ecosystems for maintaining and monitoring Big Data Computing Systems.
