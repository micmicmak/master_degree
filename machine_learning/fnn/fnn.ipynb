{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af315c6a",
   "metadata": {},
   "source": [
    "Programming Assignment 2\n",
    "         \n",
    "To be submitted via canvas, just as Programming Assignment 1\n",
    "\n",
    "This program builds a two-layer neural network for the Iris dataset.\n",
    "The first layer is a relu layer with 10 units, and the second one is \n",
    "a softmax layer. The network structure is specified in the \"train\" function.\n",
    "\n",
    "The parameters are learned using SGD.  The forward propagation and backward \n",
    "propagation are carried out in the \"compute_neural_net_loss\" function.  The codes\n",
    "for the propagations are deleted.  Your task is to fill in the missing codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c21a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this exercise, we are going to work with a two-layer neural network\n",
    "# first layer is a relu layer with 10 units, and second one is a softmax layer.\n",
    "# randomly initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a80c5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5245f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data sets\n",
    "IRIS_TRAINING = \"./iris_training.csv\"\n",
    "IRIS_TEST = \"./iris_test.csv\"\n",
    "\n",
    "def get_data():\n",
    "    # Load datasets.\n",
    "    train_data = np.genfromtxt(IRIS_TRAINING, skip_header=1, \n",
    "        dtype=float, delimiter=',') \n",
    "    test_data = np.genfromtxt(IRIS_TEST, skip_header=1, \n",
    "        dtype=float, delimiter=',') \n",
    "    train_x = train_data[:, :4]\n",
    "    train_y = train_data[:, 4].astype(np.int64)\n",
    "    test_x = test_data[:, :4]\n",
    "    test_y = test_data[:, 4].astype(np.int64)\n",
    "\n",
    "    return train_x, train_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73fcb110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neural_net_loss(params, X, y, reg=0.0):\n",
    "    \"\"\"\n",
    "    Neural network loss function.\n",
    "    Inputs:\n",
    "    - params: dictionary of parameters, including \"W1\", \"b1\", \"W2\", \"b2\"\n",
    "    - X: N x D array of training data. Each row is a D-dimensional point.\n",
    "    - y: 1-d array of shape (N, ) for the training labels.\n",
    "\n",
    "    Returns:\n",
    "    - loss: the softmax loss with regularization\n",
    "    - grads: dictionary of gradients for the parameters in params\n",
    "    \"\"\"\n",
    "    # Unpack variables from the params dictionary\n",
    "    W1, b1 = params['W1'], params['b1']\n",
    "    W2, b2 = params['W2'], params['b2']\n",
    "    N, D = X.shape\n",
    "\n",
    "    loss = 0.0\n",
    "    grads = {}\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Finish the forward pass, and compute the loss. This should include  #\n",
    "    # both the data loss and L2 regularization for W1 and W2. Store the result  #\n",
    "    # in the variable loss, which should be a scalar. Use the Softmax           #\n",
    "    # classifier loss. So that your results match ours, multiply the            #\n",
    "    # regularization loss by 0.5                                                #\n",
    "    #############################################################################\n",
    "    scores = None\n",
    "    z = np.dot(X, W1) + b1\n",
    "    u = np.maximum(z, 0) # ReLU\n",
    "    scores = np.dot(u , W2) + b2\n",
    "    \n",
    "    out = np.exp(scores)\n",
    "    out /= np.sum(out, axis=1).reshape(N, 1)\n",
    "    \n",
    "    # compute softmax loss\n",
    "    loss -= np.sum(np.log(out[np.arange(N), y]))\n",
    "    loss /= N\n",
    "    loss += 0.5 * reg * (np.sum(W1**2) + np.sum(W2**2))\n",
    "    \n",
    "    #############################################################################\n",
    "    #                              END OF YOUR CODE                             #\n",
    "    #############################################################################\n",
    "\n",
    "    #############################################################################\n",
    "    # TODO: Compute the backward pass, computing the derivatives of the weights #\n",
    "    # and biases. Store the results in the grads dictionary. For example,       #\n",
    "    # grads['W1'] should store the gradient on W1, and be a matrix of same size #\n",
    "    #############################################################################\n",
    "    dout = np.copy(out)\n",
    "    dout[np.arange(N), y] -= 1\n",
    "    du = np.dot(dout, W2.T)\n",
    "    dz = np.dot(dout, W2.T) * (z > 0)\n",
    "    \n",
    "    # compute gradient for parameters\n",
    "    dW1 = np.dot(X.T, dz) / N\n",
    "    db1 = np.sum(dz, axis=0) / N\n",
    "\n",
    "    dW2 = np.dot(u.T, dout) / N\n",
    "    db2 = np.sum(dout, axis=0) / N\n",
    "    \n",
    "    # add reg term\n",
    "    dW2 += reg * W2\n",
    "    dW1 += reg * W1\n",
    "    #############################################################################\n",
    "    #                          END OF YOUR CODE                                 #\n",
    "    #############################################################################\n",
    "    grads['W1'] = dW1\n",
    "    grads['W2'] = dW2\n",
    "    grads['b1'] = db1\n",
    "    grads['b2'] = db2\n",
    "    \n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f922c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(params, X):\n",
    "    \"\"\"\n",
    "    Use the trained weights of this linear classifier to predict labels for\n",
    "    data points.\n",
    "\n",
    "    Inputs:\n",
    "    - params: dictionary of parameters, including \"W1\", \"b1\", \"W2\", \"b2\"\n",
    "    - X: N x D array of training data. Each row is a D-dimensional point.\n",
    "\n",
    "    Returns:\n",
    "    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N, and each element is an integer giving the predicted\n",
    "      class.\n",
    "    \"\"\"\n",
    "    # Unpack variables from the params dictionary\n",
    "    W1, b1 = params['W1'], params['b1']\n",
    "    W2, b2 = params['W2'], params['b2']\n",
    "\n",
    "    y_pred = np.zeros(X.shape[1])\n",
    "   \n",
    "    relu = lambda x: x * (x > 0)\n",
    "    z1 = np.dot(X,W1)+b1\n",
    "    u1 = relu(z1)\n",
    "    z2 = np.dot(u1,W2)+b2\n",
    "    y_pred = np.argmax(z2, axis=1)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd2cd078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(ylabel, y_pred):\n",
    "    return np.mean(ylabel == y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "189bf789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_update(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Perform sgd update for parameters in params.\n",
    "    \"\"\"\n",
    "    for key in params:\n",
    "        params[key] += -learning_rate * grads[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5de1c4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_gradient():\n",
    "    \"\"\"\n",
    "    Function to validate the implementation of gradient computation.\n",
    "    Should be used together with gradient_check.py.\n",
    "    This is a useful thing to do when you implement your own gradient\n",
    "    calculation methods.\n",
    "    It is not required for this assignment.\n",
    "    \"\"\"\n",
    "    from gradient_check import eval_numerical_gradient, rel_error\n",
    "    # randomly initialize W\n",
    "    dim = 4\n",
    "    num_classes = 4\n",
    "    num_inputs = 5\n",
    "    params = {}\n",
    "    std = 0.001\n",
    "    params['W1'] = std * np.random.randn(dim, 10)\n",
    "    params['b1'] = np.zeros(10)\n",
    "    params['W2'] = std * np.random.randn(10, num_classes)\n",
    "    params['b2'] = np.zeros(num_classes)\n",
    "\n",
    "    X = np.random.randn(num_inputs, dim)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "\n",
    "    loss, grads = compute_neural_net_loss(params, X, y, reg=0.1)\n",
    "    # these should all be less than 1e-8 or so\n",
    "    for param_name in params:\n",
    "        f = lambda W: compute_neural_net_loss(params, X, y, reg=0.1)[0]\n",
    "        param_grad_num = eval_numerical_gradient(f, params[param_name], verbose=False)\n",
    "        print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9594e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, Xtest, ytest, learning_rate=1e-3, reg=1e-5, epochs=100, batch_size=20):\n",
    "    num_train, dim = X.shape\n",
    "    num_classes = np.max(y) + 1 # assume y takes values 0...K-1 where K is number of classes\n",
    "    num_iters_per_epoch = int(math.floor(1.0*num_train/batch_size))\n",
    "    \n",
    "    # In this exercise, we are going to work with a two-layer neural network\n",
    "    # first layer is a relu layer with 10 units, and second one is a softmax layer.\n",
    "    # randomly initialize parameters\n",
    "    params = {}\n",
    "    std = 0.001\n",
    "    params['W1'] = std * np.random.randn(dim, 10)\n",
    "    params['b1'] = np.zeros(10)\n",
    "    params['W2'] = std * np.random.randn(10, num_classes)\n",
    "    params['b2'] = np.zeros(num_classes)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        perm_idx = np.random.permutation(num_train)\n",
    "        # perform mini-batch SGD update\n",
    "        for it in range(num_iters_per_epoch):\n",
    "            idx = perm_idx[it*batch_size:(it+1)*batch_size]\n",
    "            batch_x = X[idx]\n",
    "            batch_y = y[idx]\n",
    "            \n",
    "            # evaluate loss and gradient\n",
    "            loss, grads = compute_neural_net_loss(params, batch_x, batch_y, reg)\n",
    "\n",
    "            # update parameters\n",
    "            sgd_update(params, grads, learning_rate)\n",
    "            \n",
    "        # evaluate and print every 10 steps\n",
    "        if epoch % 10 == 0:\n",
    "            train_acc = acc(y, predict(params, X))\n",
    "            test_acc = acc(ytest, predict(params, Xtest))\n",
    "            print('Epoch %4d: loss = %.2f, train_acc = %.4f, test_acc = %.4f' \\\n",
    "                % (epoch, loss, train_acc, test_acc))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660b21b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: loss = 1.10, train_acc = 0.3500, test_acc = 0.2667\n",
      "Epoch   10: loss = 0.75, train_acc = 0.7000, test_acc = 0.5333\n",
      "Epoch   20: loss = 0.46, train_acc = 0.7083, test_acc = 0.5667\n",
      "Epoch   30: loss = 0.46, train_acc = 0.6417, test_acc = 0.7333\n",
      "Epoch   40: loss = 0.24, train_acc = 0.9000, test_acc = 0.8667\n",
      "Epoch   50: loss = 0.59, train_acc = 0.7083, test_acc = 0.7667\n",
      "Epoch   60: loss = 0.17, train_acc = 0.9500, test_acc = 0.9667\n",
      "Epoch   70: loss = 0.12, train_acc = 0.9667, test_acc = 0.9667\n",
      "Epoch   80: loss = 0.14, train_acc = 0.9417, test_acc = 0.9667\n",
      "Epoch   90: loss = 0.13, train_acc = 0.9667, test_acc = 0.9667\n",
      "Epoch  100: loss = 0.10, train_acc = 0.9417, test_acc = 0.9667\n",
      "Epoch  110: loss = 0.12, train_acc = 0.9750, test_acc = 0.9667\n",
      "Epoch  120: loss = 0.11, train_acc = 0.9250, test_acc = 0.9333\n",
      "Epoch  130: loss = 0.07, train_acc = 0.9833, test_acc = 0.9667\n",
      "Epoch  140: loss = 0.07, train_acc = 0.9833, test_acc = 0.9667\n",
      "Epoch  150: loss = 0.05, train_acc = 0.9583, test_acc = 0.9333\n",
      "Epoch  160: loss = 0.05, train_acc = 0.9917, test_acc = 0.9667\n",
      "Epoch  170: loss = 0.36, train_acc = 0.9250, test_acc = 0.9333\n",
      "Epoch  180: loss = 0.12, train_acc = 0.9667, test_acc = 0.9667\n",
      "Epoch  190: loss = 0.08, train_acc = 0.9833, test_acc = 0.9667\n",
      "New Samples, Class Predictions:    [1 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# validate_gradient()  # don't worry about this.\n",
    "# sys.exit()\n",
    "\n",
    "max_epochs = 200\n",
    "batch_size = 20\n",
    "learning_rate = 0.1\n",
    "reg = 0.001\n",
    "\n",
    "# get training and testing data\n",
    "train_x, train_y, test_x, test_y = get_data()\n",
    "params = train(train_x, train_y, test_x, test_y, learning_rate, reg, max_epochs, batch_size)\n",
    "\n",
    "# Classify two new flower samples.\n",
    "def new_samples():\n",
    "    return np.array(\n",
    "      [[6.4, 3.2, 4.5, 1.5],\n",
    "       [5.8, 3.1, 5.0, 1.7]], dtype=np.float32)\n",
    "new_x = new_samples()\n",
    "predictions = predict(params, new_x)\n",
    "\n",
    "print(\"New Samples, Class Predictions:    {}\\n\".format(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
